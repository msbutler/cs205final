{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f07c8493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p37/gpu_cuda10.0/lib/python3.7/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "987367f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p37/gpu_cuda10.0/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7554149c",
   "metadata": {},
   "source": [
    "Should take around 3 minutes to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b807b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data from Memory\n",
      "Loaded!\n"
     ]
    }
   ],
   "source": [
    "h_dim = 1000\n",
    "v_dim = 750\n",
    "print(\"Loading Data from Memory\")\n",
    "\n",
    "root = \"Train\"\n",
    "label_flood_dir = os.path.join(root,'Labeled','Flooded','image')\n",
    "label_nonflood_dir = os.path.join(root,'Labeled','Non-Flooded','image')\n",
    "flooded_img = []\n",
    "nonflooded_img = []\n",
    "\n",
    "for file in os.listdir(label_flood_dir):\n",
    "    image = Image.open(os.path.join(label_flood_dir, file))\n",
    "    flooded_img.append(np.array(image.resize((h_dim,v_dim))))\n",
    "    \n",
    "for file in os.listdir(label_nonflood_dir):\n",
    "    image = Image.open(os.path.join(label_nonflood_dir, file))\n",
    "    nonflooded_img.append(np.array(image.resize((h_dim,v_dim))))\n",
    "print(\"Loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f26586fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flooded Image Shape: (750, 1000, 3)\n",
      "Non_Flooded Image Shape: (750, 1000, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(398, 750, 1000, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Flooded Image Shape: {}\".format(flooded_img[0].shape))\n",
    "print(\"Non_Flooded Image Shape: {}\".format(nonflooded_img[0].shape))\n",
    "\n",
    "data_img = np.vstack((np.array(flooded_img), np.array(nonflooded_img))) / 255.\n",
    "data_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3ee3dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data len 51; n is 51 ;train size 40, test size 11\n",
      "Data len 347; n is 347 ;train size 277, test size 70\n",
      "Training Indices len 317\n",
      "Testing Indices len 81\n"
     ]
    }
   ],
   "source": [
    "from utils import train_test_split\n",
    "train_idx, test_idx, train_labels, test_labels = train_test_split(flooded_img, nonflooded_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f174344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolutional layer\n",
    "# x the input \n",
    "# shape is dimension of input\n",
    "def conv_layer(x, shape,stride):\n",
    "    weights = tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "\n",
    "    bias = tf.Variable(tf.constant(0.05, shape=[shape[-1]]))\n",
    "\n",
    "    out = tf.nn.conv2d(input=x, filters=weights, strides=stride, padding='VALID')\n",
    "    out += bias\n",
    "    return out\n",
    "\n",
    "# pooling layer\n",
    "def max_pool(x, k=2):\n",
    "\n",
    "    out = tf.nn.max_pool(value=x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='VALID')\n",
    "    return out\n",
    "\n",
    "# fully connected layer\n",
    "def fully_connected_layer(x, shape):\n",
    "    weights = tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "\n",
    "    bias = tf.Variable(tf.constant(0.05, shape=[shape[1]]))\n",
    "\n",
    "    out = tf.matmul(a=x, b=weights)\n",
    "    out += bias\n",
    "    return out\n",
    "\n",
    "# flatten layer\n",
    "def flatten_layer(x):\n",
    "    \n",
    "    size = x.get_shape()[1:4].num_elements()\n",
    "    out = tf.reshape(x, [-1,size])\n",
    "    return out, size\n",
    "\n",
    "# relu\n",
    "relu = lambda x: tf.nn.relu(features=x)\n",
    "\n",
    "# softmax\n",
    "softmax = lambda x: tf.nn.softmax(logits=x)\n",
    "\n",
    "# sigmoid\n",
    "sigmoid = lambda x: tf.nn.sigmoid(x)\n",
    "\n",
    "# batch norm\n",
    "batch_norm = lambda x: tf.layers.batch_normalization(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "23470b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define CNN\n",
    "def toy_model(x,show_dim = False):\n",
    "    \n",
    "    # Six convolutional layers with max pool and ReLU\n",
    "    \n",
    "        #shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "    def conv_block(x,in_channels,out_channels, show_dim = False, stride = 2, kernel_h=10,kernel_w =10,):\n",
    "        shape = [kernel_h,kernel_w,in_channels,out_channels]\n",
    "        x = conv_layer(x, shape,stride)\n",
    "        x = relu(x)\n",
    "        x = batch_norm(x)\n",
    "        x = max_pool(x, k=2)\n",
    "        if show_dim:\n",
    "            print(x.shape)\n",
    "        return x\n",
    "    \n",
    "    x = conv_block(x,3,16,show_dim)\n",
    "\n",
    "    x = conv_block(x,16,16,show_dim)\n",
    "        \n",
    "    x = conv_block(x,16,32,show_dim)\n",
    "        \n",
    "    #x = conv_block(x,32,64,show_dim)\n",
    "    \n",
    "    #x = conv_block(x,64,64,show_dim)\n",
    "\n",
    "    # flatten output and put through a fully connected layer\n",
    "    flat1, size1 = flatten_layer(x)\n",
    "    print(flat1.shape)\n",
    "    fc1 = fully_connected_layer(flat1, [size1, 64])\n",
    "    fc1 = relu(fc1)\n",
    "    if show_dim:\n",
    "        print(fc1.shape)\n",
    "\n",
    "    fc2 = fully_connected_layer(fc1, [64, 1])\n",
    "    if show_dim:\n",
    "        print(fc2.shape)\n",
    "\n",
    "    return fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8858647",
   "metadata": {},
   "source": [
    "# dont pad, larger kernel size, do reduce feature space and less comp expensive\n",
    "\n",
    "- with unbalanced training, change the loss such that the minority gets much more weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37a79a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def sharpen(p):\n",
    "#     T = 0.5\n",
    "#     pred = p**(1./T)/(p**(1./T) + (1.-p)**(1./T))\n",
    "#     return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49b8f703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 185, 248, 16)\n",
      "(?, 44, 60, 16)\n",
      "(?, 9, 13, 32)\n",
      "(?, 3744)\n",
      "(?, 64)\n",
      "(?, 1)\n"
     ]
    }
   ],
   "source": [
    "# define inputs\n",
    "x = tf.placeholder(tf.float32, [None, v_dim, h_dim, 3])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "y_train_true = np.array(train_labels).reshape(-1,1)\n",
    "y_test_true = np.array(test_labels).reshape(-1,1)\n",
    "\n",
    "# run model with placeholder tensors\n",
    "pred = toy_model(x,show_dim = True)\n",
    "\n",
    "# sharpen\n",
    "# pred = sharpen(pred)\n",
    "\n",
    "# define loss\n",
    "cross_entropy = tf.losses.sigmoid_cross_entropy(logits=pred, multi_class_labels=y)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# define accuracy\n",
    "pred_class = tf.round(sigmoid(pred))\n",
    "pred_correct = tf.equal(pred_class, tf.cast(y, tf.float32))\n",
    "accuracy = tf.reduce_mean(tf.cast(pred_correct, tf.float32))\n",
    "\n",
    "# define optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(cost)\n",
    "\n",
    "# initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "training_iters = 450\n",
    "batch_size = 16 #len(train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c10a870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Loss= 0.568574, Training Accuracy= 0.82237 ,Testing Accuracy: 0.86420\n",
      "Iter 1, Loss= 0.932041, Training Accuracy= 0.86842 ,Testing Accuracy: 0.86420\n",
      "Iter 2, Loss= 0.362519, Training Accuracy= 0.86842 ,Testing Accuracy: 0.86420\n",
      "Iter 3, Loss= 0.430766, Training Accuracy= 0.86842 ,Testing Accuracy: 0.86420\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    train_accuracy = []\n",
    "    test_accuracy = []\n",
    "    best_acc = 0.\n",
    "    summary_writer = tf.summary.FileWriter('./Output', sess.graph)\n",
    "    num_batches = len(train_idx)//batch_size\n",
    "    \n",
    "    for i in range(training_iters):\n",
    "        \n",
    "        # Reset metrics\n",
    "        loss_total = 0\n",
    "        acc_total = 0\n",
    "        train_results = []\n",
    "   \n",
    "        # Run optimization \n",
    "        # Calculate batch loss and accuracy\n",
    "        for batch in range(num_batches):\n",
    "            batch_x = data_img[train_idx,:,:,:][batch*batch_size:min((batch+1)*batch_size,len(train_idx))]\n",
    "            batch_y = y_train_true[batch*batch_size:min((batch+1)*batch_size,len(y_train_true))]    \n",
    "\n",
    "            feed_dict={x: batch_x, y: batch_y}\n",
    "            opt = sess.run(optimizer, feed_dict=feed_dict)\n",
    "            loss, acc, pred_labels = sess.run([cost, accuracy, pred_class], feed_dict=feed_dict)\n",
    "            loss_total += loss\n",
    "            acc_total += acc\n",
    "            train_results.append(pred_labels)\n",
    "\n",
    "        # Average metrics\n",
    "        ave_loss = loss_total/num_batches\n",
    "        ave_acc = acc_total/num_batches\n",
    "\n",
    "        # Calculate accuracy for all test images\n",
    "        valid_loss, test_acc, test_results = sess.run([cost, accuracy, pred_class],\n",
    "                                feed_dict={x: data_img[test_idx,:,:,:], y : y_test_true})\n",
    "        \n",
    "        # Update metrics\n",
    "        train_loss.append(ave_loss)\n",
    "        test_loss.append(valid_loss)\n",
    "        train_accuracy.append(ave_acc)\n",
    "        test_accuracy.append(test_acc)\n",
    "        if test_acc > best_acc:\n",
    "            best_model_train_labels = tf.stack(tf.reshape(tf.stack(train_results),[-1,1])).eval()\n",
    "            best_model_test_labels = test_results\n",
    "            best_acc = test_acc\n",
    "        \n",
    "        # Print metrics\n",
    "        print(\"Iter \" + str(i) + \", Loss= \" + \\\n",
    "                      \"{:.6f}\".format(ave_loss) + \", Training Accuracy= \" + \\\n",
    "                      \"{:.5f}\".format(ave_acc)+ \\\n",
    "                      \" ,Testing Accuracy:\",\"{:.5f}\".format(test_acc))\n",
    "    summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391a1ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a48e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278b614f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd16f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "for title, data in {\"Loss\":train_loss,\"Train Accuracy\": train_accuracy, \"Test Accuracy\": test_accuracy}.items():\n",
    "    plt.plot(data)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02592675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p37)",
   "language": "python",
   "name": "conda_tensorflow_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
