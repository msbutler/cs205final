{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00000-c9411c2c-499e-47f2-8e90-48359f30a042",
        "deepnote_cell_type": "code"
      },
      "source": "import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n\nimport numpy as np\nimport skimage as sk\nfrom skimage import transform\nimport random\nfrom PIL import Image\nimport os\nimport matplotlib.pyplot as plt\n\nfrom utils import train_test_split",
      "execution_count": 1,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p37/gpu_cuda10.0/lib/python3.7/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n\nWARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p37/gpu_cuda10.0/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\nInstructions for updating:\nnon-resource variables are not supported in the long term\n"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "Matplotlib is building the font cache using fc-list. This may take a moment.\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00001-6ee2e928-83b8-449a-a873-d56ff0f32e53",
        "deepnote_cell_type": "code"
      },
      "source": "np.mean([1.2])",
      "execution_count": 2,
      "outputs": [
        {
          "data": {
            "text/plain": "1.2"
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00002-5fd46f44-d261-427e-b7a4-312aafd58e01",
        "deepnote_cell_type": "code"
      },
      "source": "def rotate_img(image):\n    random_degree = random.uniform(-25, 25) #25% from left or right\n    return sk.transform.rotate(image, random_degree)\n\ndef noise_img(image):\n    return sk.util.random_noise(image)",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00003-6f80fd6f-6c44-4198-a551-dbe47c35624c",
        "deepnote_cell_type": "code"
      },
      "source": "with tf.Session() as sess:\n    devices = sess.list_devices()\n    for d in devices:\n        print(d)",
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 5862138923887689640)\n_DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 10617289109640153054)\n_DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 2388582426510636676)\n_DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:0, GPU, 3247505408, 8892568903897373140)\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00004-4952f9a1-0bf2-4dd7-9b2e-9d23e70a7a5f",
        "deepnote_cell_type": "code"
      },
      "source": "flooded_img = []\nnonflooded_img = []\nunlabeled_img = []\n\nh_dim  = 1000\nv_dim = 750\n\nroot = \"Train\"\nflood_dir = os.path.join(root,'Labeled/Flooded/image/')\nnonflood_dir = os.path.join(root,'Labeled/Non-Flooded/image/')\nunlabel_dir = os.path.join(root,'Unlabeled/image/')\n\nfor file in os.listdir(flood_dir):\n    image = Image.open(os.path.join(flood_dir, file))\n    image = np.array(image.resize((h_dim,v_dim)))\n    flooded_img.append(rotate_img(image))\n\nfor file in os.listdir(nonflood_dir):\n    image = Image.open(os.path.join(nonflood_dir, file))\n    image = np.array(image.resize((h_dim,v_dim)))\n    nonflooded_img.append(rotate_img(image))\n    \nfor file in os.listdir(unlabel_dir):\n    image = Image.open(os.path.join(unlabel_dir, file))\n    image = np.array(image.resize((h_dim,v_dim)))\n    unlabeled_img.append(rotate_img(image))\n    unlabeled_img.append(rotate_img(image))",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00005-c3f39d0e-db59-4411-8499-017ef4e209bd",
        "deepnote_cell_type": "code"
      },
      "source": "print(\"Flooded Image Shape: {}\".format(flooded_img[0].shape))\nprint(\"Non_Flooded Image Shape: {}\".format(nonflooded_img[0].shape))\nprint(\"Unlabeled Image Shape: {}\".format(unlabeled_img[0].shape))",
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Flooded Image Shape: (750, 1000, 3)\nNon_Flooded Image Shape: (750, 1000, 3)\nUnlabeled Image Shape: (750, 1000, 3)\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00006-4f946783-0b1b-4223-92ec-9844d0049d0b",
        "deepnote_cell_type": "code"
      },
      "source": "data_img = np.vstack((np.array(flooded_img), np.array(nonflooded_img))) / 255.\ndata_img.shape",
      "execution_count": 7,
      "outputs": [
        {
          "data": {
            "text/plain": "(397, 750, 1000, 3)"
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00007-ed70c2c7-f64d-429d-bfdd-cffc193c8882",
        "deepnote_cell_type": "code"
      },
      "source": "unlabeled_img = np.array(unlabeled_img) /255.\nunlabeled_img.shape",
      "execution_count": 8,
      "outputs": [
        {
          "data": {
            "text/plain": "(366, 750, 1000, 3)"
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00008-47ff1934-a6bb-4968-a993-93bb1b4f1f8d",
        "deepnote_cell_type": "code"
      },
      "source": "#train_idx = np.array([np.arange(7),np.arange(10,17)]).flatten()\n#test_idx = np.array([np.arange(7,10),np.arange(17,20)]).flatten()\n\ntest = False\n\n#n is number images from each class (flooded or non flooded)\nif test == True:\n    n = 20\nelse:\n    n = min(len(flooded_img),len(nonflooded_img))\n\n\nidxs = train_test_split(n,flooded_img,nonflooded_img,unlabeled_img)\nlabel_train_idx, label_test_idx, train_labels, test_labels, unlabel_train_idx, unlabel_test_idx = idxs",
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Training Index: [  0   1   2   3   4   5   6   7   8   9  11  12  13  14  15  18  19  20\n  22  23  24  25  27  28  29  30  31  36  37  38  39  41  42  43  44  45\n  46  48  49  50  57  73  77  90 100 107 108 118 121 123 138 141 149 161\n 170 179 188 192 193 198 205 215 217 219 237 248 252 265 304 310 312 320\n 330 335 348 352 360 365 381 386]\nTesting Index: [ 10  16  17  21  26  32  33  34  35  40  47 112 172 195 203 228 269 284\n 323 325 346 364]\nUnlabeled Training Index: [ 0  1  2  3  4  5  8  9 10 11 12 13 14 16 17 18 19 20 21 22 23 24 25 26\n 27 31 32 33 36 37 39 40 41 42 43 44 45 47 48 50]\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### CNN CODE",
      "metadata": {
        "cell_id": "00009-52c19d5d-5b6f-4c76-8cdf-5f4797fe3232",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00010-2168eb0b-3232-4bb1-9a86-4b0739f22e7f",
        "deepnote_cell_type": "code"
      },
      "source": "# convolutional layer\ndef conv_layer(x, shape):\n\n    weights = tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n    bias = tf.Variable(tf.constant(0.05, shape=[shape[-1]]))\n\n    out = tf.nn.conv2d(input=x, filters=weights, strides=[1,1,1,1], padding='SAME')\n    out += bias\n    return out\n\n# pooling layer\ndef max_pool(x, k=2):\n\n    out = tf.nn.max_pool(value=x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n    return out\n\n# fully connected layer\ndef fully_connected_layer(x, shape):\n\n    weights = tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n    bias = tf.Variable(tf.constant(0.05, shape=[shape[1]]))\n\n    out = tf.matmul(a=x, b=weights)\n    out += bias\n    return out\n\n# flatten layer\ndef flatten_layer(x):\n    \n    size = x.get_shape()[1:4].num_elements()\n    out = tf.reshape(x, [-1,size])\n    return out, size\n\n# relu\nrelu = lambda x: tf.nn.relu(features=x)\n\n# softmax\nsoftmax = lambda x: tf.nn.softmax(logits=x)\n\n# batch norm\nbatch_norm = lambda x: tf.nn.batch_normalization(x)",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00011-b4c7d667-ad2c-452b-9531-a66101ab4d7c",
        "deepnote_cell_type": "code"
      },
      "source": "#shape = [filter_size, filter_size, num_input_channels, num_filters]\n\n# define CNN\ndef toy_model(x):\n\n    # Six convolutional layers with max pool and ReLU\n    shape0 = [5, 5, 3, 3]\n    conv0 = conv_layer(x, shape0)\n    conv0 = relu(conv0)\n    conv0 = batch_norm(conv0)\n    conv0 = max_pool(conv0, k=2)\n\n    shape1 = [5, 5, 3, 3]\n    conv1 = conv_layer(conv0, shape1)\n    conv1 = relu(conv1)\n    conv1 = batch_norm(conv1)\n    conv1 = max_pool(conv1, k=2)\n\n    shape2 = [5, 5, 3, 3]\n    conv2 = conv_layer(conv1, shape2)\n    conv2 = relu(conv2)\n    conv2 = batch_norm(conv2)\n    conv2 = max_pool(conv2, k=2)\n\n    shape3 = [5, 5, 3, 1]\n    conv3 = conv_layer(conv2, shape3)\n    conv3 = relu(conv3)\n    conv3 = batch_norm(conv3)\n    conv3 = max_pool(conv3, k=2)\n\n    shape4 = [5, 5, 1, 1]\n    conv4 = conv_layer(conv3, shape4)\n    conv4 = relu(conv4)\n    conv4 = batch_norm(conv4)\n    conv4 = max_pool(conv4, k=2)\n\n    shape5 = [5, 5, 1, 1]\n    conv5 = conv_layer(conv4, shape5)\n    conv5 = relu(conv5)\n    conv5 = batch_norm(conv5)\n    conv5 = max_pool(conv5, k=2)\n\n    # flatten output and put through a fully connected layer\n    flat1, size1 = flatten_layer(conv5)\n    fc1 = fully_connected_layer(flat1, [size1, 64])\n    fc1 = relu(fc1)\n\n    fc2 = fully_connected_layer(fc1, [64, 1])\n    out = softmax(fc2)\n\n    return out",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00012-b85abf7e-fc25-43ab-a2c9-236ed677654c",
        "deepnote_cell_type": "code"
      },
      "source": "def sharpen(p):\n    T = 0.5\n    pred = p**(1./T)/(p**(1./T) + (1.-p)**(1./T))\n    return pred",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00013-1cb03929-e884-4721-a3b9-52efe6544e0b",
        "deepnote_cell_type": "code"
      },
      "source": "def generate_guess_label(pred_u_raw, k):\n    # guess label = average prediction over k augmentations of same image\n    # num_images = pred_u_raw.shape[0].value / k # Throws error of NoneType and int since pred_u_raw.shape[0].value is None\n    \n    try:\n        num_images = int(pred_u_raw.shape[0].value / k)\n\n        idx = 0\n        temp_labels = []\n        for i in range(num_images):\n            temp_labels.append(tf.reduce_mean(pred_u_raw[idx:idx+k]))\n            idx += k\n\n        # repeat label for each augmentation\n        guess_labels = tf.repeat(tf.stack(temp_labels), k)\n\n        # reshape and remove gradient tracking\n        guess_labels = tf.reshape(guess_labels, (-1,1))\n        guess_labels = tf.stop_gradient(guess_labels)\n\n        return guess_labels\n\n    except TypeError:\n      \n      return pred_u_raw",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00014-18ba1d9b-88c7-4057-80eb-83f6326de7d3",
        "deepnote_cell_type": "code"
      },
      "source": "# define inputs\nhdim = data_img[0].shape[0]\nvdim = data_img[0].shape[1]\nx = tf.placeholder(tf.float32, [None, hdim, vdim, 3], 'x') # labeled images (augmented)\nu = tf.placeholder(tf.float32, [None, hdim, vdim, 3], 'u') # unlabeled images (augmented)\ny = tf.placeholder(tf.float32, [None, 1], 'y') # labels\ntrain_labels = np.array(train_labels).reshape(-1,1)\ntest_labels = np.array(test_labels).reshape(-1,1)\nk = 2 # augment images k times\n\n# Google paper section 3.5 says 100 is a good place to start for w_unlabeled\n# Google paper also suggests ramping up value to 100 over first 16,000 epochs\nw_unlabeled = 100. \n\n# run model with placeholder tensors (feed forward pass)\npred_x = toy_model(x)\npred_u_raw = toy_model(u)\n\n# calculate guess labels for unlabeled images \npred_u = generate_guess_label(pred_u_raw, k) # average predictions across same unlabelled images\n\n# sharpen guess labels \npred_u = sharpen(pred_u)\n\n# define loss\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=pred_x, labels=y)\nlabeled_loss = tf.reduce_mean(cross_entropy)\nunlabeled_loss = tf.nn.l2_loss(pred_u - pred_u_raw)\ncost = labeled_loss + w_unlabeled*unlabeled_loss\n\n# define accuracy\npred_correct = tf.equal(tf.argmax(pred_x, 1), tf.argmax(y, 1))\naccuracy = tf.reduce_mean(tf.cast(pred_correct, tf.float32))\n\n# define optimizer\noptimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n\n# initialize variables\ninit = tf.global_variables_initializer()\ntraining_iters = 5\nbatch_size = 4 #len(train_idx)",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Train Model",
      "metadata": {
        "cell_id": "00015-ec89594c-d9f7-4a28-9b80-390376ed16a3",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00016-6678dcec-4993-4099-b271-9dcc00ef6313",
        "deepnote_cell_type": "code"
      },
      "source": "# train model\nwith tf.Session() as sess:\n    sess.run(init)\n    train_loss = []\n    test_loss = []\n    train_accuracy = []\n    test_accuracy = []\n    summary_writer = tf.summary.FileWriter('./Output', sess.graph)\n    num_batches = len(label_train_idx)//batch_size\n\n    \n    for i in range(training_iters):\n        \n        # Reset metrics\n        loss_total = 0\n        acc_total = 0\n   \n        # Run optimization \n        # Calculate batch loss and accuracy\n        for batch in range(num_batches):\n            batch_x = data_img[label_train_idx,:,:,:][batch*batch_size:min((batch+1)*batch_size, len(label_train_idx))]\n            batch_u = unlabeled_img[unlabel_train_idx,:,:,:][batch*k*batch_size:min((batch+1)*k*batch_size, len(unlabel_train_idx))]\n            batch_y = train_labels[batch*batch_size:min((batch+1)*batch_size, len(train_labels))]\n\n            feed_dict={x: batch_x, u: batch_u, y: batch_y}\n            opt = sess.run(optimizer, feed_dict=feed_dict)\n            loss, acc = sess.run([cost, accuracy], feed_dict=feed_dict)\n            loss_total += loss\n            acc_total += acc\n\n        # Average metrics\n        ave_loss = loss_total/num_batches\n        ave_acc = acc_total/num_batches\n\n        print(\"Iter \" + str(i) + \", Loss= \" + \\\n                      \"{:.6f}\".format(ave_loss) + \", Training Accuracy= \" + \\\n                      \"{:.5f}\".format(ave_acc))\n\n        # Calculate accuracy for all test images\n        test_acc,valid_loss = sess.run([accuracy,cost],\n                                feed_dict={x: data_img[label_test_idx,:,:,:], u: unlabeled_img[unlabel_test_idx,:,:,:], y : test_labels})\n        train_loss.append(ave_loss)\n        test_loss.append(valid_loss)\n        train_accuracy.append(ave_acc)\n        test_accuracy.append(test_acc)\n        print(\"Testing Accuracy:\",\"{:.5f}\".format(test_acc))\n    summary_writer.close()",
      "execution_count": 21,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Iter 0, Loss= 0.000000, Training Accuracy= 1.00000\nTesting Accuracy: 1.00000\nIter 1, Loss= 0.000000, Training Accuracy= 1.00000\nTesting Accuracy: 1.00000\nIter 2, Loss= 0.000000, Training Accuracy= 1.00000\nTesting Accuracy: 1.00000\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00017-1e25c9a4-adb4-4ddb-9aa6-f99c36bb0c89",
        "deepnote_cell_type": "code"
      },
      "source": "for title, data in {\"Loss\":train_loss,\"Train Accuracy\": train_accuracy, \"Test Accuracy\": test_accuracy}.items():\n    plt.plot(data)\n    plt.title(title)\n    plt.xlabel(Iteration)\n    plt.xticks(range(training_iters))\n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=33603a1c-a2e3-4132-9bc0-d2ce278458e7' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "deepnote_notebook_id": "8adeeecf-006d-4ae5-93da-09179c95980c",
    "deepnote": {},
    "deepnote_execution_queue": []
  }
}